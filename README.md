Project Title: Image-to-Image and Video-to-Video Translation using Pix2Pix and CycleGAN

Team Members: Santiago Valencia Sanchez, Jue Hou, Nikhil Khandekar, Azaan Barlas

Final Results: https://www.youtube.com/shorts/hspKupwXvsM and https://www.youtube.com/shorts/hspKupwXvsM

Overview:
Our project explores the power of image-to-image and video-to-video translation using Pix2Pix and CycleGAN. We focused on three main areas of application: satellite-to-map images, facade segmentation, and artistic style transfer on videos.

Methodology:
We re-implemented Pix2Pix and CycleGAN and used non-ML image processing methods to help transform and stitch large images and perform video-to-video translation. 
For the map section, we input large satellite map images and broke them up into smaller patches with overlap (512x512pix with 50 overlapping). After processing these patches through the model, we used a stitching method to combine them and blend the overlap using gaussian-laplacian pyramids. 
For videos, two original video clips are stored in the 'video inputs' folders within the data directory. We separated the original video clips into individual frames, and stored the frames as separate images in their corresponding subfolders "city_dark_frames" and "vangogh_frames". Furthermore, we resized each frame to 512x512 before inputting them into the models. We utilized various techniques to improve the video quality, including histogram matching, gaussian-laplacian pyramids blending, and denoising by appling non-local means filter to remove noise. The resulting images at each stage of the image processing were saved in their respective subfolders - 'hist_output', 'denoise_output', and 'laplacian_output'. The final processed images, after passing through all three methods, were saved in the 'hist_denoise_laplacian_output' folder and can now be assembled into a video.

Results:
Our results showed that CycleGAN outperformed Pix2Pix in both the map and artistic video sections. For the map section, both models generated maps that closely resembled Google maps. However, for artistic style transfer, CycleGAN performed better than Pix2Pix, maintaining the structures of objects in the pictures while having less noise. Pix2Pix required additional processing techniques to improve the video quality, while CycleGAN did not. Within the data folder, a captivating video was produced from the frames generated by our models, which can be found in the 'fake_video' subfolder. This video showcases the raw, unprocessed output of the models. However, the true magic lies in the 'final_video' folder, where we meticulously applied proper image processing techniques to the frames, resulting in a stunning final product that represents the pinnacle of our achievement. Witness the remarkable quality we attained after the completion of the entire processing pipeline in this folder. Moreover, the side-by-side comparison with the original unprocessed video is a testament to the success of our image processing strategies.

Conclusion:
Our project highlights the effectiveness of image-to-image and video-to-video translation using Pix2Pix and CycleGAN. By incorporating non-ML image processing methods, we were able to achieve better results with fewer resources. Our findings suggest that CycleGAN is a better choice for artistic style transfer and that Pix2Pix requires additional processing techniques for video output.
