{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJdsdL7hcdxL",
    "outputId": "fffa32a4-39a1-47e2-f009-777fbe181e56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeXodxhFHKmP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/Folder_all_stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIpajeu6BeN_"
   },
   "outputs": [],
   "source": [
    "# load all library and packages\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from string import Template\n",
    "import numpy as np\n",
    "import os\n",
    "from google.colab.patches import cv2_imshow\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jJPjes9883f"
   },
   "outputs": [],
   "source": [
    "# with zipfile.ZipFile('video inputs/city_dark_frames.zip', 'r') as zip_ref:\n",
    "#    zip_ref.extractall('city_dark_frames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8NdHsiPc0CZ"
   },
   "source": [
    "# Video processing methods\n",
    "including \n",
    "(1) split the video into frames; \n",
    "(2) stitch frames back to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YQCt3JtHLFs"
   },
   "outputs": [],
   "source": [
    "# video to frames\n",
    "def video2imageFolder(input_file, output_path):\n",
    "    '''\n",
    "    Extracts the frames from an input video file\n",
    "    and saves them as separate frames in an output directory.\n",
    "    Input:\n",
    "        input_file: Input video file.\n",
    "        output_path: Output directorys.\n",
    "    Output:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    cap = cv2.VideoCapture()\n",
    "    cap.open(input_file)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Failed to open input video\")\n",
    "\n",
    "    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    frame_idx = 0\n",
    "\n",
    "    while frame_idx < frame_count:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print (\"Failed to get the frame {}\".format(frame_idx))\n",
    "            continue\n",
    "\n",
    "        out_name = os.path.join(output_path, 'f{:04d}.jpg'.format(frame_idx+1))\n",
    "        ret = cv2.imwrite(out_name, frame)\n",
    "        if not ret:\n",
    "            print (\"Failed to write the frame {}\".format(frame_idx))\n",
    "            continue\n",
    "\n",
    "        frame_idx += 1\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAbV_tHoHG2s"
   },
   "source": [
    "# Image pre-processing and stitch methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYsOee54HKi4"
   },
   "outputs": [],
   "source": [
    "def break_up_panorama(image_path, folder_name, frame_size, overlap_size):\n",
    "    image = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "    filename = os.path.basename(image_path)\n",
    "    image_name = os.path.splitext(filename)[0]\n",
    "\n",
    "    rows = height // (frame_size - overlap_size) + 1\n",
    "    cols = width // (frame_size - overlap_size) + 1\n",
    "\n",
    "    # create folder for split images\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    print(folder_name)\n",
    "\n",
    "    # loop over rows and columns and save each frame_size x frame_size image\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            y_min = i * (frame_size - overlap_size)\n",
    "            y_max = min(y_min + frame_size, height)\n",
    "            x_min = j * (frame_size - overlap_size)\n",
    "            x_max = min(x_min + frame_size, width)\n",
    "\n",
    "            split_img = image.crop((x_min, y_min, x_max, y_max))\n",
    "\n",
    "            filename = f\"panorama_{i}_{j}.jpg\"\n",
    "            filepath = os.path.join(folder_name, filename)\n",
    "            split_img.save(filepath)\n",
    "\n",
    "\n",
    "def stitch_panorama(image_path, folder_name, reconstructed_path, frame_size, overlap_size):\n",
    "    # Feed this function the same parameters as in 'break_up_panorama'\n",
    "\n",
    "    original_image = Image.open(image_path)\n",
    "\n",
    "    filename = os.path.basename(image_path)\n",
    "    image_name = os.path.splitext(filename)[0]\n",
    "    # folder_name = f\"{image_name}_split_panorama\"\n",
    "\n",
    "    # calculate the number of rows and columns\n",
    "    width, height = original_image.size\n",
    "    rows = height // (frame_size - overlap_size) + 1\n",
    "    cols = width // (frame_size - overlap_size) + 1\n",
    "\n",
    "    # create a new empty image with the same dimensions as the original\n",
    "    stitched_image = Image.new('RGB', (width, height))\n",
    "\n",
    "    # loop over the split images and paste them onto the new image\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            # load the split image\n",
    "            # filename = f\"panorama_{i}_{j}.jpg\"\n",
    "            filename = f\"panorama_{i}_{j}_fake_B.png\"\n",
    "            filepath = os.path.join(folder_name, filename)\n",
    "            split_image = Image.open(filepath)\n",
    "\n",
    "            # calculate the paste location\n",
    "            y_min = i * (frame_size - overlap_size)\n",
    "            y_max = min(y_min + frame_size, height)\n",
    "            x_min = j * (frame_size - overlap_size)\n",
    "            x_max = min(x_min + frame_size, width)\n",
    "\n",
    "            # paste the split image onto the new image\n",
    "            stitched_image.paste(split_image, (x_min, y_min))\n",
    "\n",
    "    # save the final stitched image to disk\n",
    "    stitched_image.save(reconstructed_path + f\"{image_name}_split_panorama.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUoBBaBieo62"
   },
   "outputs": [],
   "source": [
    "# frames to video\n",
    "def imageFolder2video(input_path, output_path, video_name, fps):\n",
    "      # create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    images = [img for img in os.listdir(input_path) if (img.endswith(\".png\") or img.endswith(\".jpg\"))]\n",
    "    images.sort()\n",
    " \n",
    "    frame = cv2.imread(os.path.join(input_path, images[0]))\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(os.path.join(output_path, video_name), fourcc, fps, (width, height))\n",
    "\n",
    "    for image in images:\n",
    "        img_path = os.path.join(input_path, image)\n",
    "        frame = cv2.imread(img_path)\n",
    "\n",
    "        if frame is None:\n",
    "            print (\"Failed to read the image {}\".format(img_path))\n",
    "            continue\n",
    "\n",
    "        video.write(frame)\n",
    "\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zD22D12MqK3v"
   },
   "source": [
    "# **The following functions are utilized to process each video frame:**\n",
    "The pipeline of video frame processing including:\n",
    "\n",
    "1.   Histogram matching is applied to adjust the pixel values of each frame to a target histogram. This improves the contrast and brightness of the frames.\n",
    "2.   Non-local means filtering is used to reduce noise while preserving edges and textures in the frames.\n",
    "3.   Laplacian blending is utilized along the left and right edges of adjacent frames to smoothly transition between them. This helps to prevent visual artifacts that may appear when frames are merged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ancITwtHQl6"
   },
   "outputs": [],
   "source": [
    "### histogram mapping method\n",
    "def hist_match(source_path, template_path):\n",
    "  \"\"\"\n",
    "  Adjust the pixel values of a grayscale image such that its histogram\n",
    "  matches that of a target image\n",
    "  \n",
    "  source: input image to be transformed\n",
    "  template: target image with the desired histogram\n",
    "  \"\"\"\n",
    "  source = cv2.imread(source_path)\n",
    "  template = cv2.imread(template_path)\n",
    "\n",
    "  old_shape = source.shape\n",
    "  source = source.ravel()\n",
    "  template = template.ravel()\n",
    "  \n",
    "  # get the set of unique pixel values and their corresponding indices and counts\n",
    "  s_values, s_idx, s_counts = np.unique(source, return_inverse=True, return_counts=True)\n",
    "  t_values, t_idx, t_counts = np.unique(template, return_inverse=True, return_counts=True)\n",
    "\n",
    "  # calculate the normalized cumulative distribution functions for the two images\n",
    "  s_quantiles = np.cumsum(s_counts).astype(np.float64)\n",
    "  s_quantiles /= s_quantiles[-1]\n",
    "  t_quantiles = np.cumsum(t_counts).astype(np.float64)\n",
    "  t_quantiles /= t_quantiles[-1]\n",
    "\n",
    "  # use linear interpolation of the quantiles to find the pixel values in the source image\n",
    "  # that correspond to the pixel values in the template image\n",
    "  interp_t_values = np.interp(s_quantiles, t_quantiles, t_values)\n",
    "\n",
    "  return interp_t_values[s_idx].reshape(old_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sr3CE-oo855N"
   },
   "outputs": [],
   "source": [
    "def hist_match_handler(frames_folder_path, output_folder_path):\n",
    "  # Create output folder if it does not exist\n",
    "  if not os.path.exists(output_folder_path):\n",
    "      os.makedirs(output_folder_path)\n",
    "\n",
    "  file_list = sorted([os.path.join(frames_folder_path, f) for f in os.listdir(frames_folder_path) if f.endswith(('.png', '.jpg'))])\n",
    "\n",
    "  # write the first image into the new folder\n",
    "  file_ = os.path.join(output_folder_path, 'f0001_hist.png')\n",
    "  cv2.imwrite(file_, cv2.imread(file_list[0]))\n",
    "\n",
    "  # it takes the first frame as template for hist_match\n",
    "  for i in range(1,len(file_list)):\n",
    "      file_ = os.path.join(output_folder_path, 'f{:04d}_hist.png'.format(i+1))\n",
    "      cv2.imwrite(file_, hist_match(file_list[i], file_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnQLYOpRO2X5"
   },
   "outputs": [],
   "source": [
    "# to be used with the denoising method in the next cell\n",
    "def fastNlMeansDenoisingColored(image, out_array, h, h, search_window, block_size):\n",
    "    # Convert image to float64 data type\n",
    "    image = image.astype(np.float64)\n",
    "\n",
    "    # Split image into color channels\n",
    "    b, g, r = cv2.split(image)\n",
    "\n",
    "    # Compute the size of the image\n",
    "    rows, cols = image.shape[:2]\n",
    "\n",
    "    # Compute padding size based on block_size\n",
    "    pad_size = block_size // 2\n",
    "\n",
    "    # Pad each color channel with zeros around the edges\n",
    "    b_padded = np.pad(b, ((pad_size, pad_size), (pad_size, pad_size)), mode='constant', constant_values=0)\n",
    "    g_padded = np.pad(g, ((pad_size, pad_size), (pad_size, pad_size)), mode='constant', constant_values=0)\n",
    "    r_padded = np.pad(r, ((pad_size, pad_size), (pad_size, pad_size)), mode='constant', constant_values=0)\n",
    "\n",
    "    # Compute weights for each pixel based on the color and distance similarity\n",
    "    weights = np.zeros((rows, cols), dtype=np.float64)\n",
    "    for i in range(pad_size, rows + pad_size):\n",
    "        for j in range(pad_size, cols + pad_size):\n",
    "            block = b_padded[i-pad_size:i+pad_size+1, j-pad_size:j+pad_size+1]\n",
    "            block_flat = block.flatten()\n",
    "            distances = np.sum((block_flat - b_padded)**2, axis=1)\n",
    "            weights[i-pad_size, j-pad_size] = np.sum(np.exp(-distances / (h**2)))\n",
    "\n",
    "    # Normalize weights so they sum to 1\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    # Apply weights to each color channel\n",
    "    b_filtered = np.zeros((rows, cols), dtype=np.float64)\n",
    "    g_filtered = np.zeros((rows, cols), dtype=np.float64)\n",
    "    r_filtered = np.zeros((rows, cols), dtype=np.float64)\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            b_block = b_padded[i:i+block_size, j:j+block_size]\n",
    "            g_block = g_padded[i:i+block_size, j:j+block_size]\n",
    "            r_block = r_padded[i:i+block_size, j:j+block_size]\n",
    "            b_filtered[i, j] = np.sum(b_block * weights[i:i+block_size, j:j+block_size])\n",
    "            g_filtered[i, j] = np.sum(g_block * weights[i:i+block_size, j:j+block_size])\n",
    "            r_filtered[i, j] = np.sum(r_block * weights[i:i+block_size, j:j+block_size])\n",
    "\n",
    "    # Merge color channels back into image\n",
    "    filtered_image = cv2.merge((b_filtered, g_filtered, r_filtered))\n",
    "\n",
    "    # Convert image back to uint8 data type\n",
    "    filtered_image = np.clip(filtered_image, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return filtered_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_nrGMzNn1T5"
   },
   "outputs": [],
   "source": [
    "### non-local means filter denoise method\n",
    "def denoise_frames(frames_folder_path, output_folder_path, h=5, \n",
    "                   search_window=15, block_size=5, temporal_window=3):\n",
    "    # Create output folder if it does not exist\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    # List all the frames in the input folder\n",
    "    frames_list = sorted(os.listdir(frames_folder_path))\n",
    "\n",
    "    # Loop through all the frames in the input folder\n",
    "    for i in range(len(frames_list)):\n",
    "        # Read the current frame image\n",
    "        current_frame = cv2.imread(os.path.join(frames_folder_path, frames_list[i]))\n",
    "\n",
    "        # Initialize an empty list to store neighboring frames\n",
    "        neighbor_frames = []\n",
    "\n",
    "        # Loop through the previous frames to get the neighboring frames\n",
    "        for j in range(max(0, i - temporal_window), i):\n",
    "            neighbor_frames.append(cv2.imread(os.path.join(frames_folder_path, frames_list[j])))\n",
    "\n",
    "        # Loop through the next frames to get the neighboring frames\n",
    "        for j in range(i + 1, min(i + temporal_window + 1, len(frames_list))):\n",
    "            neighbor_frames.append(cv2.imread(os.path.join(frames_folder_path, frames_list[j])))\n",
    "\n",
    "        # Apply non-local means filter to remove noise in the current frame\n",
    "        denoised = fastNlMeansDenoisingColored(current_frame, None, h, h, search_window, block_size)\n",
    "\n",
    "        # Initialize an empty list to store the denoised neighboring frames\n",
    "        denoised_neighbor_frames = []\n",
    "\n",
    "        # Apply non-local means filter to remove noise in the neighboring frames\n",
    "        for neighbor_frame in neighbor_frames:\n",
    "            denoised_neighbor = fastNlMeansDenoisingColored(neighbor_frame, None, h, h, search_window, block_size)\n",
    "\n",
    "            # Add the denoised neighboring frames to a list\n",
    "            denoised_neighbor_frames.append(denoised_neighbor)\n",
    "\n",
    "        # Take the median of the denoised neighboring frames to get a single denoised neighboring frame\n",
    "        if len(denoised_neighbor_frames) > 0:\n",
    "            denoised_neighbor = np.median(denoised_neighbor_frames, axis=0)\n",
    "\n",
    "            # Convert color format if needed\n",
    "            #if denoised_neighbor.ndim == 3 and denoised_neighbor.shape[2] == 3:\n",
    "            if denoised_neighbor.dtype == 'float64':\n",
    "                denoised_neighbor = cv2.convertScaleAbs(denoised_neighbor)\n",
    "\n",
    "            # Apply median blur to remove noise in the denoised neighboring frame\n",
    "            denoised_neighbor = cv2.medianBlur(denoised_neighbor, 3)\n",
    "\n",
    "            # Combine the denoised current frame and the denoised neighboring frame\n",
    "            denoised = cv2.addWeighted(denoised, 0.5, denoised_neighbor, 0.5, 0)\n",
    "\n",
    "        # Save the denoised image to output folder\n",
    "        # cv2.imwrite(os.path.join(output_folder_path, frames_list[i]), denoised)\n",
    "        output_filename = 'f{:04d}_denoise.png'.format(i+1)\n",
    "        cv2.imwrite(os.path.join(output_folder_path, output_filename), denoised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4eR6Js7ncX-"
   },
   "outputs": [],
   "source": [
    "### laplacian blending method on left/right edge\n",
    "def laplacian_blend(image1, image2, overlap_size):\n",
    "    # image 1 is left side, image 2 is right side, return image2blend\n",
    "    \n",
    "    # Transpose images to blend left edge (same logic as blending top edge)\n",
    "    image1 = np.transpose(image1, (1, 0, 2))\n",
    "    image2 = np.transpose(image2, (1, 0, 2))\n",
    "\n",
    "    # Generate Gaussian and Laplacian pyramids for both images\n",
    "    gaussian1 = image1.copy()\n",
    "    gaussian2 = image2.copy()\n",
    "    laplacian1 = [gaussian1]\n",
    "    laplacian2 = [gaussian2]\n",
    "\n",
    "    num_levels = int(np.floor(np.log2(min(gaussian1.shape[0], gaussian2.shape[0], overlap_size))) - 4)\n",
    "\n",
    "    for i in range(num_levels):\n",
    "        gaussian1_down = cv2.pyrDown(gaussian1)\n",
    "        gaussian2_down = cv2.pyrDown(gaussian2)\n",
    "        laplacian1.append(cv2.subtract(gaussian1, cv2.pyrUp(gaussian1_down)))\n",
    "        laplacian2.append(cv2.subtract(gaussian2, cv2.pyrUp(gaussian2_down)))\n",
    "        gaussian1 = gaussian1_down\n",
    "        gaussian2 = gaussian2_down\n",
    "\n",
    "    # Combine the Laplacian pyramids of the two images\n",
    "    blended_pyramid = []\n",
    "    for i in range(num_levels, 0, -1):\n",
    "        size = (laplacian1[i - 1].shape[1], laplacian1[i - 1].shape[0])\n",
    "        if i == num_levels:\n",
    "            blended_pyramid.append(cv2.addWeighted(laplacian1[i - 1], 0.5, laplacian2[i - 1], 0.5, 0))\n",
    "        else:\n",
    "            overlap_region1 = blended_pyramid[-1][:, -overlap_size:]\n",
    "            overlap_region2 = laplacian2[i - 1][:, :overlap_size]\n",
    "            alpha = np.linspace(0, 1, overlap_size)\n",
    "            overlap_region = cv2.addWeighted(overlap_region1, 1 - alpha, overlap_region2, alpha, 0)\n",
    "            blended_pyramid.append(np.concatenate((laplacian1[i - 1][:, :-overlap_size], overlap_region), axis=1))\n",
    "\n",
    "    # Combine the non-overlapping regions of the two images\n",
    "    blended_image = blended_pyramid[-1]\n",
    "    for i in range(num_levels - 1, -1, -1):\n",
    "        blended_image = cv2.pyrUp(blended_image)\n",
    "        if i == num_levels - 1:\n",
    "            blended_image = cv2.addWeighted(laplacian1[i], 0.5, laplacian2[i], 0.5, 0)\n",
    "        else:\n",
    "            overlap_region1 = blended_image[:, -overlap_size:]\n",
    "            overlap_region2 = blended_pyramid[i][:, :overlap_size]\n",
    "            alpha = np.linspace(0, 1, overlap_size)\n",
    "            overlap_region = cv2.addWeighted(overlap_region1, 1 - alpha, overlap_region2, alpha, 0)\n",
    "            blended_image = np.concatenate((blended_pyramid[i][:, :-overlap_size], overlap_region), axis=1)\n",
    "            blended_image = cv2.addWeighted(laplacian1[i], 0.5, blended_image, 0.5, 0)\n",
    "\n",
    "    # Crop the overlapping region\n",
    "    blended_image = np.transpose(blended_image, (1, 0, 2))\n",
    "    \n",
    "    return blended_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snyyfP7zm_9i"
   },
   "outputs": [],
   "source": [
    "def laplacian_blend_handler(folder_path, overlap_size, output_folder_path):\n",
    "    image_files = sorted([f for f in os.listdir(folder_path) if f.endswith(('.png','.jpg'))])\n",
    "    num_images = len(image_files)\n",
    "    if num_images < 2:\n",
    "        print(\"Error: folder must contain at least two image files\")\n",
    "        return\n",
    "\n",
    "    # create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    for i in range(num_images - 1):\n",
    "        # extract image order numbers from file names\n",
    "        image1_order = int(image_files[i][1:5])\n",
    "        image2_order = int(image_files[i+1][1:5])\n",
    "        output_filename = 'f{:04d}_blend.png'.format(image2_order)\n",
    "\n",
    "        # load images and blend them\n",
    "        image1 = cv2.imread(os.path.join(folder_path, image_files[i]))\n",
    "        image2 = cv2.imread(os.path.join(folder_path, image_files[i+1]))\n",
    "        blended_image = laplacian_blend(image1, image2, overlap_size)\n",
    "\n",
    "        # save blended image to output folder\n",
    "        cv2.imwrite(os.path.join(output_folder_path, output_filename), blended_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygKD9i4_m_xt"
   },
   "outputs": [],
   "source": [
    "### homography method\n",
    "# but we don't use this method in our final version after testing\n",
    "\n",
    "def homography(original_pic, transformed_pic):\n",
    "\n",
    "  # Load images\n",
    "  img_a = original_pic\n",
    "  img_b = transformed_pic\n",
    "\n",
    "  # Detect and extract keypoints from both images\n",
    "  orb = cv2.ORB_create(nfeatures=1000)\n",
    "  kp_a, des_a = orb.detectAndCompute(img_a, None)\n",
    "  kp_b, des_b = orb.detectAndCompute(img_b, None)\n",
    "\n",
    "  # Match keypoints using brute-force matcher\n",
    "  bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "  matches = bf.match(des_a, des_b)\n",
    "\n",
    "  # Sort matches by distance\n",
    "  matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "  # Get the top N matches\n",
    "  n_matches = 50\n",
    "  matches = matches[:n_matches]\n",
    "\n",
    "  # Get corresponding keypoints from both images\n",
    "  pts_a = np.float32([kp_a[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "  pts_b = np.float32([kp_b[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "  # Compute homography matrix\n",
    "  homography, _ = cv2.findHomography(pts_b, pts_a, cv2.RANSAC)\n",
    "\n",
    "  # Apply homography to image B\n",
    "  img_b_aligned = cv2.warpPerspective(img_b, homography, (img_a.shape[1], img_a.shape[0]))\n",
    "\n",
    "  # Display result\n",
    "  cv2_imshow(img_a)\n",
    "  cv2_imshow(img_b)\n",
    "  cv2_imshow(img_b_aligned)\n",
    "  cv2.waitKey(0)\n",
    "  cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6Tk-buyM9vz"
   },
   "source": [
    "# **Apply method on the 'city' video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDAZLEuIe18n"
   },
   "outputs": [],
   "source": [
    "# some pro-processing\n",
    "# cropping video to square for better output\n",
    "# load input video\n",
    "cap = cv2.VideoCapture('video inputs/city_water.mp4')\n",
    "\n",
    "# get video dimensions\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# calculate square dimensions\n",
    "dim = min(width, height)\n",
    "\n",
    "# calculate top-left corner coordinates of the square\n",
    "x = int((width - dim) / 2)\n",
    "y = int((height - dim) / 2)\n",
    "\n",
    "# create output video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('city_water_centered.mp4', fourcc, 30.0, (dim, dim))\n",
    "\n",
    "# loop through frames and reframe each frame as a square\n",
    "while cap.isOpened():\n",
    "     ret, frame = cap.read()\n",
    "     if not ret:\n",
    "         break\n",
    "\n",
    "     # crop frame to square dimensions\n",
    "     frame = frame[y:y+dim, x:x+dim]\n",
    "\n",
    "     # resize frame to output dimensions\n",
    "     frame = cv2.resize(frame, (dim, dim))\n",
    "\n",
    "     # write frame to output video\n",
    "     out.write(frame)\n",
    "\n",
    "     # display output frame\n",
    "     cv2_imshow(frame)\n",
    "     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "         break\n",
    "\n",
    "# release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# move all fake images to another filder\n",
    "# Create the directory to move the files to\n",
    "# if not os.path.exists('fake_images'):\n",
    "#     os.mkdir('fake_images')\n",
    "# # directory = 'results/monet2photo_pretrained/test_latest/images'\n",
    "# directory = 'frames/monet'\n",
    "\n",
    "# # Loop over all files in the current directory\n",
    "# for filename in os.listdir(directory):\n",
    "#     # Check if the filename contains the string 'fake'\n",
    "#     print(filename)\n",
    "#     if 'fake' in filename:\n",
    "#         # Create the full file path\n",
    "#         file_path = os.path.join(directory, filename)\n",
    "#         # Check if the file exists in the destination directory\n",
    "#         if not os.path.exists(os.path.join('fake_images', filename)):\n",
    "#             # Move the file to the 'fake_images' directory\n",
    "#             shutil.move(file_path, 'fake_images')\n",
    "#             print(f\"Moved {filename} to fake_images directory\")\n",
    "#         else:\n",
    "#             print(f\"{filename} already exists in fake_images directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hjWc4wAwpOq"
   },
   "source": [
    "# **First application: apply Pix2Pix model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhNzUTw2Zehh"
   },
   "outputs": [],
   "source": [
    "# make video from synthesized images w/o any image processing\n",
    "imageFolder2video(input_path='city_dark_frames/dark_images', \n",
    "                  output_path='city_dark_frames/fake_video', \n",
    "                  video_name='fake_video.mp4', \n",
    "                  fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQ6NTXLOps3v"
   },
   "outputs": [],
   "source": [
    "# Histogram matching\n",
    "hist_match_handler(frames_folder_path='city_dark_frames/dark_images', \n",
    "                   output_folder_path='city_dark_frames/hist_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X--1oP3ToAB_"
   },
   "outputs": [],
   "source": [
    "# Denoising\n",
    "denoise_frames(frames_folder_path='city_dark_frames/dark_images',\n",
    "               output_folder_path='city_dark_frames/denoise_output',\n",
    "               h=5, search_window=15, block_size=5, temporal_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dgE7lAvGFGj"
   },
   "outputs": [],
   "source": [
    "# Laplacian blending\n",
    "laplacian_blend_handler(folder_path='city_dark_frames/dark_images', \n",
    "                        overlap_size=50, \n",
    "                        output_folder_path='city_dark_frames/laplacian_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPfW6xHrT4z6"
   },
   "outputs": [],
   "source": [
    "# Sequentially apply these 3 methods by the order of histogram matching --> Denoising --> Laplacian Blending\n",
    "# histogram matching frame alreay generated, we use these output as input of denoising to proceed\n",
    "\n",
    "# Denoising after Histogram matching\n",
    "denoise_frames(frames_folder_path='city_dark_frames/hist_output',\n",
    "               output_folder_path='city_dark_frames/hist_denoise_output',\n",
    "               h=5, search_window=15, block_size=5, temporal_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgtSDgqPY3gD"
   },
   "outputs": [],
   "source": [
    "# Laplacian blending after denosing\n",
    "laplacian_blend_handler(folder_path='city_dark_frames/hist_denoise_output', \n",
    "                        overlap_size=50, \n",
    "                        output_folder_path='city_dark_frames/hist_denoise_laplacian_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzZCj0CEZSMK"
   },
   "outputs": [],
   "source": [
    "# make final video from frames w image processing\n",
    "imageFolder2video(input_path='city_dark_frames/hist_denoise_laplacian_output', \n",
    "                  output_path='city_dark_frames/final_video', \n",
    "                  video_name='final_video.mp4',\n",
    "                  fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I403N2cxw90r"
   },
   "source": [
    "# **Second application: apply cycleGAN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNX_Nd-KxXxa"
   },
   "outputs": [],
   "source": [
    "# make video from synthesized images w/o any image processing\n",
    "imageFolder2video(input_path='vangogh_frames/vangogh_output', \n",
    "                  output_path='vangogh_frames/vangogh_synthesized_video', \n",
    "                  video_name='vangogh_synthesized_video.mp4', \n",
    "                  fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szAm0V0Nyhis"
   },
   "outputs": [],
   "source": [
    "# Histogram matching\n",
    "hist_match_handler(frames_folder_path='vangogh_frames/vangogh_output',\n",
    "                   output_folder_path='vangogh_frames/hist_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4mgBtSW0R1m"
   },
   "outputs": [],
   "source": [
    "# Denoising\n",
    "denoise_frames(frames_folder_path='vangogh_frames/vangogh_output',\n",
    "               output_folder_path='vangogh_frames/denoise_output',\n",
    "               h=5, search_window=15, block_size=5, temporal_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuccIU-Q2zR7"
   },
   "outputs": [],
   "source": [
    "# Laplacian blending\n",
    "laplacian_blend_handler(folder_path='vangogh_frames/vangogh_output', \n",
    "                        overlap_size=50, \n",
    "                        output_folder_path='vangogh_frames/laplacian_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4lFoOEZ20H9"
   },
   "outputs": [],
   "source": [
    "# # Sequentially apply these 3 methods by the order of histogram matching --> Denoising --> Laplacian Blending\n",
    "# histogram matching frame alreay generated, we use these output as input of denoising to proceed\n",
    "\n",
    "# Denoising after Histogram matching\n",
    "denoise_frames(frames_folder_path='vangogh_frames/hist_output',\n",
    "               output_folder_path='vangogh_frames/hist_denoise_output',\n",
    "               h=5, search_window=15, block_size=5, temporal_window=3)\n",
    "\n",
    "# Laplacian blending after denosing\n",
    "laplacian_blend_handler(folder_path='vangogh_frames/hist_denoise_output', \n",
    "                        overlap_size=50, \n",
    "                        output_folder_path='vangogh_frames/hist_denoise_laplacian_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWlArGlA3Hhz"
   },
   "outputs": [],
   "source": [
    "# make final video from frames w image processing\n",
    "imageFolder2video(input_path='vangogh_frames/hist_denoise_laplacian_output', \n",
    "                  output_path='vangogh_frames/final_video', \n",
    "                  video_name='final_video.mp4',\n",
    "                  fps=30)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
