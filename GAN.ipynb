{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/sketchdata.zip', 'r') as ref:\n",
        "  ref.extractall('/content/sketchdata')"
      ],
      "metadata": {
        "id": "qCjjNNLxKDIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-pjzBlUJcYb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = '/content/sketchdata/photo'\n",
        "sketch_dir = '/content/sketchdata/original_sketch'\n",
        "x = np.load(dir)"
      ],
      "metadata": {
        "id": "NX0rm56JT7iN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "a667a126-0f63-4bc7-bbdf-7fe9a34abd0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8d866b97474b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/sketchdata/photo'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msketch_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/sketchdata/original_sketch'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/sketchdata'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load images in\n",
        "class images(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    image = self.X[index]\n",
        "    X = self.transform(image)\n",
        "    return X"
      ],
      "metadata": {
        "id": "uWeai4aaQHYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.RandomResizedCrop(size = (224, 224)),\n",
        "                                transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "brwZlbG9St-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eM6Q8B10SuBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lDwMVulmSuEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b85WlyC9KTp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    self.first_stage = torch.nn.Conv2d(224, 224, 32)\n",
        "\n",
        "    self.second_stage = torch.nn.Conv2d(28, 28, 256)\n",
        "    self.third_stage = torch.nn.Conv2d(56, 56, 128)\n",
        "    self.fourth_stage = torch.nn.Conv2d(112, 112, 64)\n",
        "    one = torch.nn.Conv2d(1,1)\n",
        "    three = torch.nn.Conv2d(3,3)\n",
        "    m = torch.nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "\n",
        "\n",
        "  def forward(self, img):\n",
        "\n",
        "    feature_maps = []\n",
        "\n",
        "    output_img = torch.zeros(())\n",
        "\n",
        "    for i in range(1, N - 1):\n",
        "      f_m = feature_maps[i]\n",
        "      upsampled_fm = m(f_m)\n",
        "      f_m_1 = feature_maps[i + 1]\n",
        "      upsampled_fm = torch.cat((f_m, upsampled_fm))\n",
        "      ones_conv = one(upsampled_fm)\n",
        "      threes_conv = three(ones_conv)\n",
        "\n",
        "      feature_maps[i + 1] =  threes_conv\n",
        "\n",
        "\n",
        "    return feature_maps[-1]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rMZDWN7cJiRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adversarial_loss(generator, discriminator, photo_data):\n",
        "    # generate fake sketches from photo input\n",
        "    sketch_data = generator(photo_data)\n",
        "\n",
        "    # compute discriminator outputs for real and fake sketches\n",
        "    real_outputs = discriminator(sketch_data)\n",
        "    fake_outputs = torch.ones_like(real_outputs)\n",
        "\n",
        "    # compute adversarial loss for generator\n",
        "    adversarial_loss = F.binary_cross_entropy_with_logits(real_outputs, fake_outputs)\n",
        "\n",
        "    return adversarial_loss"
      ],
      "metadata": {
        "id": "LORqM-kdKUWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    self.first_layer = torch.nn.Conv2d(3, 32, 3, stride = 2, padding = 1)\n",
        "    self.batch1 = torch.nn.BatchNorm2d(32, eps=1e-10)\n",
        "    self.second_layer = torch.nn.Conv2d(32, 32, 3, stride = 2, padding = 1)\n",
        "    self.batch2 = torch.nn.BatchNorm2d(32, eps=1e-10)\n",
        "    self.third_layer = torch.nn.Conv2d(32, 64, 3, stride = 2, padding = 1)\n",
        "    self.batch3 = torch.nn.BatchNorm2d(64, eps=1e-10)\n",
        "    self.fourth_layer = torch.nn.Conv2d(64, 64, 3, stride = 2, padding = 1)\n",
        "    self.batch4 = torch.nn.BatchNorm2d(64, eps=1e-10)\n",
        "    self.fifth_layer = torch.nn.Conv2d(64, 128, 3, stride = 2, padding = 1)\n",
        "    self.batch5 = torch.nn.BatchNorm2d(128, eps=1e-10)\n",
        "    self.sixth_layer = torch.nn.Conv2d(128, 1, 3, stride = 1, padding = 1)\n",
        "    self.batch6 = torch.nn.BatchNorm2d(1, eps=1e-10)\n",
        "    self.sigmoid = torch.nn.sigmoid()\n",
        "  \n",
        "  def forward(self, img):\n",
        "\n",
        "    first = F.relu(self.batch1(self.first_layer(img)))\n",
        "    second = F.relu(self.batch2(self.second_layer(first)))\n",
        "    third = F.relu(self.batch3(self.third_layer(second)))\n",
        "    fourth = F.relu(self.batch4(self.fourth_layer(third)))\n",
        "    fifth = F.relu(self.batch5(self.fifth_layer(fourth)))\n",
        "    sixth = F.relu(self.batch6(self.sixth_layer(fifth)))\n",
        "    output = self.sigmoid(sixth)\n",
        "\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "9ziLh2qChpz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KcNX9p-BNtJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rwxpi8WwCW3f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}